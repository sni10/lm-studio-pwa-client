# LM Studio PWA Client

Lightweight Progressive Web App client for [LM Studio](https://lmstudio.ai/) local AI server.

## Features

- ğŸš€ **Single HTML file** - no build process required
- ğŸ“± **PWA ready** - install as mobile app  
- ğŸ”„ **Real-time streaming** - live responses with thinking process
- ğŸ’¾ **Persistent storage** - saves chat history and settings
- ğŸ¨ **Mobile optimized** - responsive design with touch support
- ğŸŒ **Multi-model support** - works with any LM Studio compatible model

## Quick Start

1. Start LM Studio server on your local network
2. Open `index.html` in your browser
3. Configure server IP/port in settings (âš™ï¸)
4. Select a model and start chatting

## Default Settings

- **Host**: `192.168.31.100`
- **Port**: `1234`
- **Max tokens**: `2048`
- **Temperature**: `0.7`

## Recent Improvements

- âœ… Model selection now persists between sessions
- âœ… Improved token counting with fallback estimation
- âœ… Better streaming response handling

---

*Simple, fast, and reliable LM Studio client that just works.*